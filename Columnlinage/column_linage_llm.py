# -*- coding: utf-8 -*-
"""Column_linage_llm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ddo9bGRnWCfDsNFbrNzRZ2AlnIeVOUN_

***
1. We will try to mimic the column lineage with LLM.
2. Spark will be used in this session and to provide the lineage etween source and target we will LLM
***
"""

import json
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, concat, lit, sum as spark_sum, avg, count
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType
from pyspark.sql import functions as F
from io import StringIO
from pyspark.sql import DataFrame
import sys

# Initialize Spark
spark = SparkSession.builder \
    .appName("DAG_Lineage_Extraction") \
    .config("spark.sql.adaptive.enabled", "false") \
    .getOrCreate()

# Create sample data
sample_data = [
    ("John", "Doe", 25, 50000.0, "Engineering"),
    ("Jane", "Smith", 30, 75000.0, "Marketing"),
    ("Bob", "Johnson", 35, 60000.0, "Engineering"),
    ("Alice", "Brown", 28, 80000.0, "Sales"),
    ("Charlie", "Wilson", 32, 70000.0, "Marketing")
]

schema = StructType([
    StructField("first_name", StringType(), True),
    StructField("last_name", StringType(), True),
    StructField("age", IntegerType(), True),
    StructField("salary", DoubleType(), True),
    StructField("department", StringType(), True)
])

# Create initial DataFrame
df_original = spark.createDataFrame(sample_data, schema)

df_original.show(10,0)

print("Original DataFrame:")
df_original.show()

# Transformation 1: Create derived columns
df_transformed = df_original \
    .withColumn("full_name", concat(col("first_name"), lit(" "), col("last_name"))) \
    .withColumn("salary_category",
                F.when(col("salary") > 70000, "High")
                .when(col("salary") > 50000, "Medium")
                .otherwise("Low")) \
    .withColumn("age_group",
                F.when(col("age") < 30, "Young")
                .otherwise("Senior"))
print("\nTransformed DataFrame:")
df_transformed.show()

df_aggregated = df_transformed \
    .groupBy("department", "salary_category") \
    .agg(
        spark_sum("salary").alias("total_salary"),
        avg("age").alias("avg_age"),
        count("*").alias("employee_count")
    )

print("\nAggregated DataFrame:")
df_aggregated.show()

"""**Function to extract dag information**"""

# Function to extract DAG information
def extract_dag_info(dataframe, stage_name):
    """Extract both logical and physical plan information"""

    # Method 1: Using explain() to capture plans
    from io import StringIO
    import sys

    # Capture explain output
    old_stdout = sys.stdout
    sys.stdout = buffer = StringIO()

    # Get extended plan information
    dataframe.explain(extended=True)
    plan_output = buffer.getvalue()

    sys.stdout = old_stdout

    # Split the explain output into logical and physical parts
    lines = plan_output.split('\n')
    logical_start = -1
    physical_start = -1

    for i, line in enumerate(lines):
        if "Parsed Logical Plan" in line or "Analyzed Logical Plan" in line:
            logical_start = i
        elif "Physical Plan" in line:
            physical_start = i
            break

    logical_plan_str = '\n'.join(lines[logical_start:physical_start]) if logical_start != -1 else "Not found"
    physical_plan_str = '\n'.join(lines[physical_start:]) if physical_start != -1 else "Not found"

    dag_info = {
        "stage_name": stage_name,
        "logical_plan": logical_plan_str,
        "physical_plan": physical_plan_str,
        "full_explain": plan_output,
        "schema": [{"name": field.name, "type": str(field.dataType)} for field in dataframe.schema.fields]
    }

    return dag_info

"""**TEXT for LLM**"""

# Function to extract transformation details for LLM
def extract_transformation_context(df_before, df_after, transformation_type):
    """Extract context that LLM can use for lineage analysis"""

    before_columns = [field.name for field in df_before.schema.fields]
    after_columns = [field.name for field in df_after.schema.fields]

    transformation_context = {
        "transformation_type": transformation_type,
        "input_columns": before_columns,
        "output_columns": after_columns,
        "new_columns": list(set(after_columns) - set(before_columns)),
        "dropped_columns": list(set(before_columns) - set(after_columns))
    }

    return transformation_context

"""**Extracted DAG information**"""

print("\n" + "="*50)
print("EXTRACTING DAG INFORMATION")
print("="*50)

original_dag = extract_dag_info(df_original, "original")
transformed_dag = extract_dag_info(df_transformed, "transformed")
aggregated_dag = extract_dag_info(df_aggregated, "aggregated")

original_dag

transformed_dag

aggregated_dag

"""**Extract transformation contexts**"""

transform_context_1 = extract_transformation_context(df_original, df_transformed, "column_derivation")
transform_context_2 = extract_transformation_context(df_transformed, df_aggregated, "aggregation")

transform_context_1

transform_context_2

"""**Prepare data for LLM processing**"""

llm_input_data = {
    "job_id": "sample_lineage_job",
    "transformations": [
        {
            "stage": "original_to_transformed",
            "context": transform_context_1,
            "dag_info": transformed_dag
        },
        {
            "stage": "transformed_to_aggregated",
            "context": transform_context_2,
            "dag_info": aggregated_dag
        }
    ]
}

"""**DAG information for LLM**"""

print("\nSample DAG Information for LLM:")
print(json.dumps(llm_input_data, indent=2)[:1000] + "...")


print("\nLogical Plan for 'transformed' stage:")
print(transformed_dag["logical_plan"][:500] + "...")

print("\nPhysical Plan for 'aggregated' stage:")
print(aggregated_dag["physical_plan"][:500] + "...")


print("\n" + "="*50)
print("DAG EXTRACTION COMPLETE")
print("="*50)

"""**Function to prepare LLM prompt**"""

def prepare_llm_prompt(transformation_data):
    """Prepare structured prompt for LLM to extract column lineage"""

    prompt = f"""
    Analyze this Spark transformation and extract column lineage:

    Transformation Type: {transformation_data['context']['transformation_type']}
    Input Columns: {transformation_data['context']['input_columns']}
    Output Columns: {transformation_data['context']['output_columns']}
    New Columns: {transformation_data['context']['new_columns']}

    Logical Plan:
    {transformation_data['dag_info']['logical_plan'][:800]}

    Extract column lineage in this format:
    {{
        "lineage": [
            {{"source_columns": ["col1", "col2"], "target_column": "derived_col", "operation": "concat"}},
            {{"source_columns": ["col3"], "target_column": "derived_col2", "operation": "case_when"}}
        ]
    }}
    """

    return prompt

sample_prompt = prepare_llm_prompt(llm_input_data["transformations"][0])

sample_prompt

"""**LLMA model**"""

!pip install transformers==4.36.0 torch accelerate bitsandbytes

from huggingface_hub import login
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import re

# Login with your token
login(new_session=False)

"""**Model loading**"""

print("Loading CodeLlama model...")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/CodeLlama-7b-Instruct-hf")
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/CodeLlama-7b-Instruct-hf",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # For Colab memory optimization
)

def process_prompt_with_codellama(prompt):
    """Process prompt through CodeLlama"""
    formatted_prompt = f"<s>[INST] {prompt.strip()} [/INST]"

    inputs = tokenizer(formatted_prompt, return_tensors="pt", truncation=True, max_length=2048)

    # Move inputs to GPU
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model.generate(
            inputs['input_ids'],
            max_new_tokens=512,
            temperature=0.1,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response_start = response.find("[/INST]") + 7
    return response[response_start:].strip()

test_response = process_prompt_with_codellama(sample_prompt)
print("LLM Response:", test_response[:200])

sample_prompt

test_response

