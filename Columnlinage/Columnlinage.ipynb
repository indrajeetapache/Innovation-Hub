{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hHKsmm_NWlYE"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, concat, lit, sum as spark_sum, avg, count\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "from pyspark.sql import functions as F\n",
        "from io import StringIO\n",
        "from pyspark.sql import DataFrame\n",
        "import sys\n",
        "import os\n",
        "os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"DAG_Lineage_Extraction\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create sample data\n",
        "sample_data = [\n",
        "    (\"John\", \"Doe\", 25, 50000.0, \"Engineering\"),\n",
        "    (\"Jane\", \"Smith\", 30, 75000.0, \"Marketing\"),\n",
        "    (\"Bob\", \"Johnson\", 35, 60000.0, \"Engineering\"),\n",
        "    (\"Alice\", \"Brown\", 28, 80000.0, \"Sales\"),\n",
        "    (\"Charlie\", \"Wilson\", 32, 70000.0, \"Marketing\")\n",
        "]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"first_name\", StringType(), True),\n",
        "    StructField(\"last_name\", StringType(), True),\n",
        "    StructField(\"age\", IntegerType(), True),\n",
        "    StructField(\"salary\", DoubleType(), True),\n",
        "    StructField(\"department\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Create initial DataFrame\n",
        "df_original = spark.createDataFrame(sample_data, schema)"
      ],
      "metadata": {
        "id": "I1GQW-cIWvBt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original DataFrame:\")\n",
        "df_original.show()\n",
        "\n",
        "# Transformation 1: Create derived columns\n",
        "df_transformed = df_original \\\n",
        "    .withColumn(\"full_name\", concat(col(\"first_name\"), lit(\" \"), col(\"last_name\"))) \\\n",
        "    .withColumn(\"salary_category\",\n",
        "                F.when(col(\"salary\") > 70000, \"High\")\n",
        "                .when(col(\"salary\") > 50000, \"Medium\")\n",
        "                .otherwise(\"Low\")) \\\n",
        "    .withColumn(\"age_group\",\n",
        "                F.when(col(\"age\") < 30, \"Young\")\n",
        "                .otherwise(\"Senior\"))\n",
        "print(\"\\nTransformed DataFrame:\")\n",
        "df_transformed.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uW_wqvBfWxDR",
        "outputId": "606a2fcd-12e1-446d-9863-6e1f95819f51"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+----------+---------+---+-------+-----------+\n",
            "|first_name|last_name|age| salary| department|\n",
            "+----------+---------+---+-------+-----------+\n",
            "|      John|      Doe| 25|50000.0|Engineering|\n",
            "|      Jane|    Smith| 30|75000.0|  Marketing|\n",
            "|       Bob|  Johnson| 35|60000.0|Engineering|\n",
            "|     Alice|    Brown| 28|80000.0|      Sales|\n",
            "|   Charlie|   Wilson| 32|70000.0|  Marketing|\n",
            "+----------+---------+---+-------+-----------+\n",
            "\n",
            "\n",
            "Transformed DataFrame:\n",
            "+----------+---------+---+-------+-----------+--------------+---------------+---------+\n",
            "|first_name|last_name|age| salary| department|     full_name|salary_category|age_group|\n",
            "+----------+---------+---+-------+-----------+--------------+---------------+---------+\n",
            "|      John|      Doe| 25|50000.0|Engineering|      John Doe|            Low|    Young|\n",
            "|      Jane|    Smith| 30|75000.0|  Marketing|    Jane Smith|           High|   Senior|\n",
            "|       Bob|  Johnson| 35|60000.0|Engineering|   Bob Johnson|         Medium|   Senior|\n",
            "|     Alice|    Brown| 28|80000.0|      Sales|   Alice Brown|           High|    Young|\n",
            "|   Charlie|   Wilson| 32|70000.0|  Marketing|Charlie Wilson|         Medium|   Senior|\n",
            "+----------+---------+---+-------+-----------+--------------+---------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_aggregated = df_transformed \\\n",
        "    .groupBy(\"department\", \"salary_category\") \\\n",
        "    .agg(\n",
        "        spark_sum(\"salary\").alias(\"total_salary\"),\n",
        "        avg(\"age\").alias(\"avg_age\"),\n",
        "        count(\"*\").alias(\"employee_count\")\n",
        "    )\n",
        "\n",
        "print(\"\\nAggregated DataFrame:\")\n",
        "df_aggregated.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Qh33a_aW3WP",
        "outputId": "8daf6d11-c0a9-4f7e-810c-cca35ec307d3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Aggregated DataFrame:\n",
            "+-----------+---------------+------------+-------+--------------+\n",
            "| department|salary_category|total_salary|avg_age|employee_count|\n",
            "+-----------+---------------+------------+-------+--------------+\n",
            "|Engineering|            Low|     50000.0|   25.0|             1|\n",
            "|      Sales|           High|     80000.0|   28.0|             1|\n",
            "|  Marketing|         Medium|     70000.0|   32.0|             1|\n",
            "|Engineering|         Medium|     60000.0|   35.0|             1|\n",
            "|  Marketing|           High|     75000.0|   30.0|             1|\n",
            "+-----------+---------------+------------+-------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract DAG information\n",
        "def extract_dag_info(dataframe, stage_name):\n",
        "    \"\"\"Extract both logical and physical plan information\"\"\"\n",
        "\n",
        "    # Method 1: Using explain() to capture plans\n",
        "    from io import StringIO\n",
        "    import sys\n",
        "\n",
        "    # Capture explain output\n",
        "    old_stdout = sys.stdout\n",
        "    sys.stdout = buffer = StringIO()\n",
        "\n",
        "    # Get extended plan information\n",
        "    dataframe.explain(extended=True)\n",
        "    plan_output = buffer.getvalue()\n",
        "\n",
        "    sys.stdout = old_stdout\n",
        "\n",
        "    # Split the explain output into logical and physical parts\n",
        "    lines = plan_output.split('\\n')\n",
        "    logical_start = -1\n",
        "    physical_start = -1\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        if \"Parsed Logical Plan\" in line or \"Analyzed Logical Plan\" in line:\n",
        "            logical_start = i\n",
        "        elif \"Physical Plan\" in line:\n",
        "            physical_start = i\n",
        "            break\n",
        "\n",
        "    logical_plan_str = '\\n'.join(lines[logical_start:physical_start]) if logical_start != -1 else \"Not found\"\n",
        "    physical_plan_str = '\\n'.join(lines[physical_start:]) if physical_start != -1 else \"Not found\"\n",
        "\n",
        "    dag_info = {\n",
        "        \"stage_name\": stage_name,\n",
        "        \"logical_plan\": logical_plan_str,\n",
        "        \"physical_plan\": physical_plan_str,\n",
        "        \"full_explain\": plan_output,\n",
        "        \"schema\": [{\"name\": field.name, \"type\": str(field.dataType)} for field in dataframe.schema.fields]\n",
        "    }\n",
        "\n",
        "    return dag_info\n"
      ],
      "metadata": {
        "id": "1XlebVZ9W6AX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract transformation details for LLM\n",
        "def extract_transformation_context(df_before, df_after, transformation_type):\n",
        "    \"\"\"Extract context that LLM can use for lineage analysis\"\"\"\n",
        "\n",
        "    before_columns = [field.name for field in df_before.schema.fields]\n",
        "    after_columns = [field.name for field in df_after.schema.fields]\n",
        "\n",
        "    transformation_context = {\n",
        "        \"transformation_type\": transformation_type,\n",
        "        \"input_columns\": before_columns,\n",
        "        \"output_columns\": after_columns,\n",
        "        \"new_columns\": list(set(after_columns) - set(before_columns)),\n",
        "        \"dropped_columns\": list(set(before_columns) - set(after_columns))\n",
        "    }\n",
        "\n",
        "    return transformation_context"
      ],
      "metadata": {
        "id": "1ea9dviRW9Iq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"EXTRACTING DAG INFORMATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "original_dag = extract_dag_info(df_original, \"original\")\n",
        "transformed_dag = extract_dag_info(df_transformed, \"transformed\")\n",
        "aggregated_dag = extract_dag_info(df_aggregated, \"aggregated\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TV8qsmtPW_o4",
        "outputId": "ca052a4a-c9b7-4649-87ac-6bd0b0f78f1b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "EXTRACTING DAG INFORMATION\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform_context_1 = extract_transformation_context(df_original, df_transformed, \"column_derivation\")\n",
        "transform_context_2 = extract_transformation_context(df_transformed, df_aggregated, \"aggregation\")"
      ],
      "metadata": {
        "id": "Ai_xDTHLXBW4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LLM processing**"
      ],
      "metadata": {
        "id": "U5daVPxgXGdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "llm_input_data = {\n",
        "    \"job_id\": \"sample_lineage_job\",\n",
        "    \"transformations\": [\n",
        "        {\n",
        "            \"stage\": \"original_to_transformed\",\n",
        "            \"context\": transform_context_1,\n",
        "            \"dag_info\": transformed_dag\n",
        "        },\n",
        "        {\n",
        "            \"stage\": \"transformed_to_aggregated\",\n",
        "            \"context\": transform_context_2,\n",
        "            \"dag_info\": aggregated_dag\n",
        "        }\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "6AqiYELyXEKZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\nSample DAG Information for LLM:\")\n",
        "print(json.dumps(llm_input_data, indent=2)[:1000] + \"...\")\n",
        "\n",
        "\n",
        "print(\"\\nLogical Plan for 'transformed' stage:\")\n",
        "print(transformed_dag[\"logical_plan\"][:500] + \"...\")\n",
        "\n",
        "print(\"\\nPhysical Plan for 'aggregated' stage:\")\n",
        "print(aggregated_dag[\"physical_plan\"][:500] + \"...\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"DAG EXTRACTION COMPLETE\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWA_MU2AXIuY",
        "outputId": "9be1ca2b-18e4-4a11-a004-e90c0ca00055"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample DAG Information for LLM:\n",
            "{\n",
            "  \"job_id\": \"sample_lineage_job\",\n",
            "  \"transformations\": [\n",
            "    {\n",
            "      \"stage\": \"original_to_transformed\",\n",
            "      \"context\": {\n",
            "        \"transformation_type\": \"column_derivation\",\n",
            "        \"input_columns\": [\n",
            "          \"first_name\",\n",
            "          \"last_name\",\n",
            "          \"age\",\n",
            "          \"salary\",\n",
            "          \"department\"\n",
            "        ],\n",
            "        \"output_columns\": [\n",
            "          \"first_name\",\n",
            "          \"last_name\",\n",
            "          \"age\",\n",
            "          \"salary\",\n",
            "          \"department\",\n",
            "          \"full_name\",\n",
            "          \"salary_category\",\n",
            "          \"age_group\"\n",
            "        ],\n",
            "        \"new_columns\": [\n",
            "          \"age_group\",\n",
            "          \"salary_category\",\n",
            "          \"full_name\"\n",
            "        ],\n",
            "        \"dropped_columns\": []\n",
            "      },\n",
            "      \"dag_info\": {\n",
            "        \"stage_name\": \"transformed\",\n",
            "        \"logical_plan\": \"== Analyzed Logical Plan ==\\nfirst_name: string, last_name: string, age: int, salary: double, department: string, full_name: string, salary_category: string, age_group: string\\nProject [first_name#0, last_name#1, age#2, salar...\n",
            "\n",
            "Logical Plan for 'transformed' stage:\n",
            "== Analyzed Logical Plan ==\n",
            "first_name: string, last_name: string, age: int, salary: double, department: string, full_name: string, salary_category: string, age_group: string\n",
            "Project [first_name#0, last_name#1, age#2, salary#3, department#4, full_name#31, salary_category#38, CASE WHEN (age#2 < 30) THEN Young ELSE Senior END AS age_group#46]\n",
            "+- Project [first_name#0, last_name#1, age#2, salary#3, department#4, full_name#31, CASE WHEN (salary#3 > cast(70000 as double)) THEN High WHEN (salary#3 > c...\n",
            "\n",
            "Physical Plan for 'aggregated' stage:\n",
            "== Physical Plan ==\n",
            "*(2) HashAggregate(keys=[department#4, salary_category#38], functions=[sum(salary#3), avg(age#2), count(1)], output=[department#4, salary_category#38, total_salary#97, avg_age#99, employee_count#101L])\n",
            "+- Exchange hashpartitioning(department#4, salary_category#38, 200), ENSURE_REQUIREMENTS, [plan_id=87]\n",
            "   +- *(1) HashAggregate(keys=[department#4, salary_category#38], functions=[partial_sum(salary#3), partial_avg(age#2), partial_count(1)], output=[department#4, salary_categor...\n",
            "\n",
            "==================================================\n",
            "DAG EXTRACTION COMPLETE\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_llm_prompt(transformation_data):\n",
        "    \"\"\"Prepare structured prompt for LLM to extract column lineage\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Analyze this Spark transformation and extract column lineage:\n",
        "\n",
        "    Transformation Type: {transformation_data['context']['transformation_type']}\n",
        "    Input Columns: {transformation_data['context']['input_columns']}\n",
        "    Output Columns: {transformation_data['context']['output_columns']}\n",
        "    New Columns: {transformation_data['context']['new_columns']}\n",
        "\n",
        "    Logical Plan:\n",
        "    {transformation_data['dag_info']['logical_plan'][:800]}\n",
        "\n",
        "    Extract column lineage in this format:\n",
        "    {{\n",
        "        \"lineage\": [\n",
        "            {{\"source_columns\": [\"col1\", \"col2\"], \"target_column\": \"derived_col\", \"operation\": \"concat\"}},\n",
        "            {{\"source_columns\": [\"col3\"], \"target_column\": \"derived_col2\", \"operation\": \"case_when\"}}\n",
        "        ]\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "RZy4dWbgXORK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.36.0 torch accelerate bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrOcC9gxXRTi",
        "outputId": "27073111-4769-44cc-e312-1f16f4cd00bb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.36.0\n",
            "  Downloading transformers-4.36.0-py3-none-any.whl.metadata (126 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.0) (0.33.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.0) (2.32.3)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.36.0)\n",
            "  Downloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.36.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.36.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.36.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.36.0) (2025.6.15)\n",
            "Downloading transformers-4.36.0-py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m130.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m125.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m111.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m112.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tokenizers, nvidia-cusolver-cu12, transformers, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.2\n",
            "    Uninstalling tokenizers-0.21.2:\n",
            "      Successfully uninstalled tokenizers-0.21.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.53.0\n",
            "    Uninstalling transformers-4.53.0:\n",
            "      Successfully uninstalled transformers-4.53.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.46.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 tokenizers-0.15.2 transformers-4.36.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch"
      ],
      "metadata": {
        "id": "oVGOghvDXUWj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "token = userdata.get('HF_TOKEN')\n",
        "login(token=token, add_to_git_credential=False)"
      ],
      "metadata": {
        "id": "cVVBw-7sXam0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading CodeLlama model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/CodeLlama-7b-Instruct-hf\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/CodeLlama-7b-Instruct-hf\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    load_in_8bit=True  # For Colab memory optimization\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "b045a9e1115a4938a34852942c3b767c",
            "63d1cd35754640e98455a5a0bfade25c",
            "a7d4e45b589b4922ab9e9584ac01bb28",
            "fc560d54ee254ac0b5a49cff5098e877",
            "9c2594f8dfda4ff987f857e2a97f7fa5",
            "828ddd2c050c4c8b987778096f24bc56",
            "e27f35aec4bd48e4903c04a423a4dcda",
            "1b6cbdc5084a4fb381f994a9a6b9905a",
            "d8383c5270404842b3e40adc041772e5",
            "67f09c5f49ec46adb8d7afc4c8ceb815",
            "e14a768a59524a90a1c41f247c2c80c1",
            "2c0e072241244a51948a89f6039f7246",
            "894cf9ab4f7b4c75b6c28e9416842cbe",
            "1a5eaf09113443ed9b9b9b016b8f7409",
            "db1f7bbc239f4004880f00ca9c1ef0ea",
            "bceab125f29a447cbe50edf5d9ecde66",
            "1ca099c59ca243b0919b02594fa1aa37",
            "cb2b2dfc49ca4f6d8f6a7113370a4380",
            "2b23602e45f54d62bbc793040e1a11a3",
            "5a3b6dbfd9564e7ca3bdd46a9ec701d3",
            "482a235fbdf04a25973f228d5f4358c4",
            "b549e716e8c144d1a27cd8caaa60a934"
          ]
        },
        "id": "YkY1IYQ9XccP",
        "outputId": "4827c65a-aca5-4b34-ca4b-b9b1dc0b96ad"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading CodeLlama model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b045a9e1115a4938a34852942c3b767c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c0e072241244a51948a89f6039f7246"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_prompt_with_codellama(prompt):\n",
        "    \"\"\"Process prompt through CodeLlama\"\"\"\n",
        "    formatted_prompt = f\"<s>[INST] {prompt.strip()} [/INST]\"\n",
        "\n",
        "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
        "\n",
        "    # Move inputs to GPU\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.1,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    response_start = response.find(\"[/INST]\") + 7\n",
        "    return response[response_start:].strip()"
      ],
      "metadata": {
        "id": "xaDXHPBqXeoY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_prompt = prepare_llm_prompt(llm_input_data[\"transformations\"][0])"
      ],
      "metadata": {
        "id": "5zVMJ3n8YmnX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_response = process_prompt_with_codellama(sample_prompt)"
      ],
      "metadata": {
        "id": "VUS1C7JLXh4S"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "C-qrqpknXkdb",
        "outputId": "85510002-132c-4fb9-d64a-85972b7402dd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The column lineage for this Spark transformation can be extracted as follows:\\n\\n{\\n    \"lineage\": [\\n        {\\n            \"source_columns\": [\"first_name\", \"last_name\"],\\n            \"target_column\": \"full_name\",\\n            \"operation\": \"concat\"\\n        },\\n        {\\n            \"source_columns\": [\"salary\"],\\n            \"target_column\": \"salary_category\",\\n            \"operation\": \"case_when\"\\n        },\\n        {\\n            \"source_columns\": [\"age\"],\\n            \"target_column\": \"age_group\",\\n            \"operation\": \"case_when\"\\n        }\\n    ]\\n}\\n\\nHere, we have three column derivations:\\n\\n1. The \"full_name\" column is derived from the \"first_name\" and \"last_name\" columns using the \"concat\" operation.\\n2. The \"salary_category\" column is derived from the \"salary\" column using the \"case_when\" operation.\\n3. The \"age_group\" column is derived from the \"age\" column using the \"case_when\" operation.\\n\\nNote that the \"case_when\" operation is used for both the \"salary_category\" and \"age_group\" columns, but the \"when\" conditions are different for each column.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mabKW7iDXnmk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}